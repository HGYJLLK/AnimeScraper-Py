#!/usr/bin/env python3
"""
girigirilove HTMLç»“æ„æ·±åº¦åˆ†æå·¥å…·
ç”¨äºç¡®å®šæ­£ç¡®çš„CSSé€‰æ‹©å™¨
"""

import sys
import os
import requests
from bs4 import BeautifulSoup
import re

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from web_scraper.utils.logger import logger


def analyze_search_page_html():
    """æ·±åº¦åˆ†ææœç´¢é¡µé¢çš„HTMLç»“æ„"""
    logger.info("ğŸ” æ·±åº¦åˆ†ææœç´¢é¡µé¢HTMLç»“æ„")
    logger.info("=" * 60)
    
    # ä½¿ç”¨å·²çŸ¥å¯å·¥ä½œçš„URL
    test_url = "https://anime.girigirilove.com/search/-------------/?wd=è¿›å‡»çš„å·¨äºº"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        logger.info(f"è¯·æ±‚URL: {test_url}")
        response = requests.get(test_url, headers=headers, timeout=15)
        logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        logger.info(f"å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
        
        if response.status_code != 200:
            logger.error("é¡µé¢è®¿é—®å¤±è´¥")
            return
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ—è¡¨å®¹å™¨
        logger.info("\n=== 1. åˆ†æé¡µé¢ç»“æ„ ===")
        
        # æŸ¥æ‰¾å¸¸è§çš„åˆ—è¡¨/å®¹å™¨å…ƒç´ 
        containers = {
            'ul elements': soup.find_all('ul'),
            'div with class containing "list"': soup.find_all('div', class_=lambda x: x and 'list' in ' '.join(x).lower() if x else False),
            'div with class containing "mac"': soup.find_all('div', class_=lambda x: x and 'mac' in ' '.join(x).lower() if x else False),
            'div with class containing "item"': soup.find_all('div', class_=lambda x: x and 'item' in ' '.join(x).lower() if x else False),
            'div with class containing "card"': soup.find_all('div', class_=lambda x: x and 'card' in ' '.join(x).lower() if x else False),
            'li elements': soup.find_all('li'),
        }
        
        for container_type, elements in containers.items():
            if elements:
                logger.info(f"\n{container_type}: {len(elements)} ä¸ª")
                for i, elem in enumerate(elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    classes = elem.get('class', [])
                    logger.info(f"  {i+1}. class: {classes}")
                    
                    # æ˜¾ç¤ºå…ƒç´ çš„å‰100ä¸ªå­—ç¬¦çš„æ–‡æœ¬å†…å®¹
                    text = elem.get_text(strip=True)
                    if text:
                        logger.info(f"     text: {text[:100]}...")
                    
                    # æŸ¥æ‰¾å†…éƒ¨çš„é“¾æ¥
                    links = elem.find_all('a', limit=3)
                    if links:
                        logger.info(f"     åŒ…å« {len(elem.find_all('a'))} ä¸ªé“¾æ¥:")
                        for j, link in enumerate(links):
                            href = link.get('href', '')
                            link_text = link.get_text(strip=True)
                            logger.info(f"       é“¾æ¥{j+1}: {href} - {link_text[:30]}")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        logger.info("\n=== 2. åˆ†ææ‰€æœ‰é“¾æ¥ ===")
        all_links = soup.find_all('a')
        logger.info(f"é¡µé¢æ€»è®¡ {len(all_links)} ä¸ªé“¾æ¥")
        
        # åˆ†ç±»é“¾æ¥
        anime_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # å¯»æ‰¾å¯èƒ½çš„åŠ¨æ¼«é“¾æ¥
            if href and any(pattern in href for pattern in ['/show/', '/v/', '/anime/', '/detail/']):
                anime_links.append((href, text))
        
        if anime_links:
            logger.success(f"æ‰¾åˆ° {len(anime_links)} ä¸ªå¯èƒ½çš„åŠ¨æ¼«é“¾æ¥:")
            for i, (href, text) in enumerate(anime_links[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                logger.info(f"  {i+1}. {href} - {text[:50]}")
        
        # 3. åˆ†æé¡µé¢ä¸­çš„ç±»å
        logger.info("\n=== 3. åˆ†ææ‰€æœ‰CSSç±»å ===")
        all_classes = set()
        for elem in soup.find_all(class_=True):
            classes = elem.get('class', [])
            all_classes.update(classes)
        
        # è¿‡æ»¤å‡ºå¯èƒ½ç›¸å…³çš„ç±»å
        relevant_classes = [cls for cls in all_classes if any(keyword in cls.lower() for keyword in 
                           ['list', 'item', 'card', 'title', 'name', 'anime', 'show', 'video', 'mac'])]
        
        if relevant_classes:
            logger.info("æ‰¾åˆ°ç›¸å…³çš„CSSç±»å:")
            for cls in sorted(relevant_classes):
                elements_with_class = soup.find_all(class_=cls)
                logger.info(f"  .{cls} ({len(elements_with_class)} ä¸ªå…ƒç´ )")
        
        # 4. ç”Ÿæˆæ¨èçš„é€‰æ‹©å™¨
        logger.info("\n=== 4. æ¨èçš„é€‰æ‹©å™¨é…ç½® ===")
        
        recommendations = []
        
        # åŸºäºæ‰¾åˆ°çš„åŠ¨æ¼«é“¾æ¥æ¨èé€‰æ‹©å™¨
        if anime_links:
            sample_link = anime_links[0][0]
            parent_elements = soup.find_all('a', href=sample_link)
            if parent_elements:
                parent = parent_elements[0].find_parent()
                if parent:
                    parent_classes = parent.get('class', [])
                    if parent_classes:
                        recommendations.append(f"subject_selector: .{' .'.join(parent_classes)}")
                        recommendations.append(f"name_selector: a")  
                        recommendations.append(f"url_selector: a")
        
        # åŸºäºæ‰¾åˆ°çš„ç±»åæ¨èé€‰æ‹©å™¨
        for cls in relevant_classes:
            if 'list' in cls.lower():
                recommendations.append(f"å¯èƒ½çš„å®¹å™¨: .{cls}")
            elif 'title' in cls.lower():
                recommendations.append(f"å¯èƒ½çš„æ ‡é¢˜: .{cls}")
            elif 'item' in cls.lower():
                recommendations.append(f"å¯èƒ½çš„é¡¹ç›®: .{cls}")
        
        if recommendations:
            logger.success("æ¨èçš„é€‰æ‹©å™¨:")
            for rec in recommendations:
                logger.info(f"  {rec}")
        
        # 5. è¾“å‡ºé¡µé¢ç‰‡æ®µç”¨äºæ‰‹åŠ¨åˆ†æ
        logger.info("\n=== 5. é¡µé¢HTMLç‰‡æ®µ (ç”¨äºæ‰‹åŠ¨åˆ†æ) ===")
        
        # å¯»æ‰¾æœ€æœ‰å¯èƒ½åŒ…å«æœç´¢ç»“æœçš„éƒ¨åˆ†
        main_content = soup.find('main') or soup.find('div', class_=lambda x: x and 'main' in ' '.join(x).lower() if x else False)
        if main_content:
            logger.info("æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼ŒHTMLç‰‡æ®µ:")
            logger.info("```html")
            print(main_content.prettify()[:2000] + "..." if len(str(main_content)) > 2000 else main_content.prettify())
            logger.info("```")
        
        return soup
        
    except Exception as e:
        logger.error(f"åˆ†æHTMLæ—¶å‡ºé”™: {e}")
        return None


def test_selector_candidates(soup):
    """æµ‹è¯•å€™é€‰é€‰æ‹©å™¨"""
    if not soup:
        return
        
    logger.info("\n=== 6. æµ‹è¯•å€™é€‰é€‰æ‹©å™¨ ===")
    
    # å€™é€‰é€‰æ‹©å™¨åˆ—è¡¨
    selector_candidates = [
        # åŸºäºå¸¸è§æ¨¡å¼
        (".mac-list li", ".mac-list-title", "a"),
        (".mac-list .mac-item", ".title", "a"), 
        ("ul.mac-list li", "a", "a"),
        (".list-item", ".title", "a"),
        (".search-result", ".title", "a"),
        (".anime-item", ".name", "a"),
        
        # æ›´é€šç”¨çš„é€‰æ‹©å™¨
        ("li", "a", "a"),
        (".item", "a", "a"),  
        ("article", "h2 a", "h2 a"),
        (".card", ".title a", ".title a"),
    ]
    
    for i, (container_sel, title_sel, url_sel) in enumerate(selector_candidates):
        logger.info(f"\næµ‹è¯•é€‰æ‹©å™¨ç»„åˆ {i+1}:")
        logger.info(f"  å®¹å™¨: {container_sel}")
        logger.info(f"  æ ‡é¢˜: {title_sel}")
        logger.info(f"  é“¾æ¥: {url_sel}")
        
        try:
            containers = soup.select(container_sel)
            logger.info(f"  æ‰¾åˆ°å®¹å™¨: {len(containers)} ä¸ª")
            
            if containers:
                # æµ‹è¯•å‰å‡ ä¸ªå®¹å™¨
                valid_results = 0
                for j, container in enumerate(containers[:5]):
                    title_elem = container.select_one(title_sel)
                    url_elem = container.select_one(url_sel)
                    
                    if title_elem and url_elem:
                        title_text = title_elem.get_text(strip=True)
                        url_href = url_elem.get('href', '')
                        
                        if title_text and url_href:
                            valid_results += 1
                            logger.info(f"    ç»“æœ{j+1}: {title_text[:30]} -> {url_href}")
                
                if valid_results > 0:
                    logger.success(f"  âœ“ æ­¤é€‰æ‹©å™¨ç»„åˆæœ‰æ•ˆ! æ‰¾åˆ° {valid_results} ä¸ªæœ‰æ•ˆç»“æœ")
                    logger.info("  å»ºè®®ä½¿ç”¨æ­¤é€‰æ‹©å™¨é…ç½®")
                    
                    return {
                        'subject_selector': container_sel,
                        'name_selector': title_sel,
                        'url_selector': url_sel,
                        'valid_results': valid_results
                    }
                else:
                    logger.warning("  âœ— æ­¤é€‰æ‹©å™¨æ— æœ‰æ•ˆç»“æœ")
        
        except Exception as e:
            logger.error(f"  âœ— é€‰æ‹©å™¨é”™è¯¯: {e}")
    
    return None


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ girigirilove HTMLæ·±åº¦åˆ†æ")
    
    # åˆ†æHTMLç»“æ„
    soup = analyze_search_page_html()
    
    # æµ‹è¯•å€™é€‰é€‰æ‹©å™¨
    if soup:
        best_selector = test_selector_candidates(soup)
        
        if best_selector:
            logger.success(f"\nğŸ‰ æ‰¾åˆ°æœ€ä½³é€‰æ‹©å™¨é…ç½®:")
            logger.info(f"subject_selector = '{best_selector['subject_selector']}'")
            logger.info(f"name_selector = '{best_selector['name_selector']}'") 
            logger.info(f"url_selector = '{best_selector['url_selector']}'")
            logger.info(f"æœ‰æ•ˆç»“æœæ•°: {best_selector['valid_results']}")
        else:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„é€‰æ‹©å™¨é…ç½®ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥HTMLç»“æ„")


if __name__ == "__main__":
    main()