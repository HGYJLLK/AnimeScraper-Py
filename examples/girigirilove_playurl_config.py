#!/usr/bin/env python3
"""
girigirilove æ’­æ”¾URLè·å–é…ç½®
ç›®æ ‡ï¼šè·å–å®é™…çš„æ’­æ”¾URLè€Œä¸æ˜¯åˆ†ç±»é¡µé¢
"""

import sys
import os
from urllib.parse import quote

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from web_scraper.core import SelectorMediaSource
from web_scraper.models import (
    SelectorSearchConfig, MediaFetchRequest, EpisodeSort,
    SelectorSubjectFormatConfig, SelectorChannelFormatConfig, MatchVideoConfig
)
from web_scraper.utils.logger import logger


def create_playurl_config():
    """
    ä¸“é—¨ç”¨äºè·å–æ’­æ”¾URLçš„é…ç½®
    ä¿®å¤å ä½ç¬¦é—®é¢˜ï¼Œä¸“æ³¨äºè·å–å®é™…æ’­æ”¾é“¾æ¥
    """
    return SelectorSearchConfig(
        # æ­£ç¡®çš„URLæ ¼å¼ï¼šä¿ç•™å ä½ç¬¦ + æŸ¥è¯¢å‚æ•°
        search_url="https://anime.girigirilove.com/search/-------------/?wd={keyword}",
        
        # æœç´¢è®¾ç½®
        search_use_only_first_word=False,
        search_remove_special=False, 
        search_use_subject_names_count=1,  # å…ˆæµ‹è¯•å•ä¸ªåç§°
        
        # è¯·æ±‚è®¾ç½®
        request_interval_seconds=2.0,
        
        # ä¸»é¢˜æ ¼å¼é…ç½® - ä¸“é—¨å¯»æ‰¾æ’­æ”¾é¡µé¢é“¾æ¥
        subject_format_config=SelectorSubjectFormatConfig(
            # å¯»æ‰¾åŒ…å«æ’­æ”¾é“¾æ¥çš„å®¹å™¨
            subject_selector="li:has(a[href*='/GV']), .public-list-box:has(a[href*='/GV']), li:has(a[href*='/show/'])",
            # è·å–é“¾æ¥æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
            name_selector="a",
            # è·å–å®é™…çš„æ’­æ”¾é¡µé¢é“¾æ¥
            url_selector="a[href*='/GV'], a[href*='/show/']"
        ),
        
        # é¢‘é“æ ¼å¼é…ç½® - å‰§é›†é¡µé¢å†…çš„å…·ä½“æ’­æ”¾é“¾æ¥
        channel_format_config=SelectorChannelFormatConfig(
            # å‰§é›†é¡µé¢ä¸­çš„æ’­æ”¾é“¾æ¥å®¹å™¨
            episode_selector=".public-list-box, .play-list li, .episode-list li, li",
            # å‰§é›†åç§°
            name_selector="a, .title, .episode-name",
            # å®é™…æ’­æ”¾é“¾æ¥
            url_selector="a[href*='/play/'], a[href*='/watch/'], a[href*='/video/'], a"
        ),
        
        # è§†é¢‘åŒ¹é…é…ç½® - åŒ¹é…çœŸå®çš„æ’­æ”¾URL
        match_video=MatchVideoConfig(
            enable_nested_url=True,
            # åŒ¹é…æ’­æ”¾ç›¸å…³çš„URLæ¨¡å¼
            match_video_url=r"(\.mp4|\.m3u8|/play/|/watch/|/video/|stream|player|/GV\d+/)",
            cookies="",
        ),
        
        default_resolution="1080P",
        default_subtitle_language="CHS"
    )


class PlayUrlGirigiriloveSource(SelectorMediaSource):
    """
    ä¸“é—¨è·å–æ’­æ”¾URLçš„æ•°æ®æº
    ä¿®å¤URLæ ¼å¼é—®é¢˜ï¼Œä¸“æ³¨æ’­æ”¾é“¾æ¥æå–
    """
    
    def __init__(self, media_source_id: str, config: SelectorSearchConfig, session=None):
        super().__init__(media_source_id, config, session)
    
    def _build_correct_search_url(self, keyword: str) -> str:
        """
        æ„å»ºæ­£ç¡®çš„æœç´¢URL
        ç¡®ä¿ä¿ç•™å ä½ç¬¦å¹¶ä½¿ç”¨æŸ¥è¯¢å‚æ•°
        """
        # URLç¼–ç å…³é”®è¯
        encoded_keyword = quote(keyword)
        
        # ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ï¼šä¿ç•™å ä½ç¬¦ + æŸ¥è¯¢å‚æ•°
        search_url = f"https://anime.girigirilove.com/search/-------------/?wd={encoded_keyword}"
        
        return search_url
    
    def _is_play_url(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯æ’­æ”¾URLè€Œä¸æ˜¯åˆ†ç±»é¡µé¢
        """
        if not url:
            return False
            
        # æ’­æ”¾URLçš„ç‰¹å¾
        play_patterns = [
            '/GV',          # GVå¼€å¤´çš„åŠ¨æ¼«ID
            '/play/',       # æ’­æ”¾é¡µé¢
            '/watch/',      # è§‚çœ‹é¡µé¢
            '/video/',      # è§†é¢‘é¡µé¢
            '/episode/',    # å‰§é›†é¡µé¢
        ]
        
        for pattern in play_patterns:
            if pattern in url:
                return True
        
        # æ’é™¤åˆ†ç±»é¡µé¢
        category_patterns = [
            '/show/2-----------',  # æ—¥ç•ªåˆ†ç±»
            '/show/21-----------', # å‰§åœºç‰ˆåˆ†ç±»
            '/label/',             # æ ‡ç­¾é¡µ
        ]
        
        for pattern in category_patterns:
            if pattern in url:
                return False
                
        return True
    
    async def search(self, search_config, query):
        """è¦†ç›–æœç´¢æ–¹æ³•ï¼Œç¡®ä¿URLæ ¼å¼æ­£ç¡®"""
        # æ„å»ºæ­£ç¡®çš„æœç´¢URL
        correct_search_url = self._build_correct_search_url(query.subject_name)
        
        # ä¸´æ—¶æ›¿æ¢é…ç½®ä¸­çš„URL
        original_url = search_config.search_url
        search_config.search_url = correct_search_url
        
        logger.info(f"ä½¿ç”¨æ­£ç¡®çš„æœç´¢URL: {search_config.search_url}")
        
        try:
            # è°ƒç”¨çˆ¶ç±»æœç´¢
            results = await super().search(search_config, query)
            
            # è¿‡æ»¤å‡ºæ’­æ”¾URL
            play_results = []
            for media in results:
                url = media.download.url if media.download else ""
                
                if self._is_play_url(url):
                    play_results.append(media)
                    logger.debug(f"âœ“ æ’­æ”¾URL: {media.original_title} -> {url}")
                else:
                    logger.debug(f"âœ— è·³è¿‡åˆ†ç±»é¡µé¢: {media.original_title} -> {url}")
            
            logger.info(f"æ‰¾åˆ°æ’­æ”¾URL: {len(play_results)}/{len(results)}")
            return play_results
            
        finally:
            # æ¢å¤åŸå§‹URL
            search_config.search_url = original_url


def test_playurl_extraction():
    """æµ‹è¯•æ’­æ”¾URLæå–åŠŸèƒ½"""
    logger.info("ğŸ¯ æµ‹è¯•æ’­æ”¾URLæå–åŠŸèƒ½")
    logger.info("=" * 60)
    
    config = create_playurl_config()
    source = PlayUrlGirigiriloveSource("girigirilove-playurl", config)
    
    # æµ‹è¯•å¤šä¸ªåŠ¨æ¼«
    test_queries = [
        "è¿›å‡»çš„å·¨äºº",
        "é¬¼ç­ä¹‹åˆƒ", 
        "ç«å½±å¿è€…",
        "æµ·è´¼ç‹",
        "æ­»äº¡ç¬”è®°"  # ä»ä¹‹å‰çš„ç»“æœçœ‹ï¼Œè¿™ä¸ªå¯èƒ½å­˜åœ¨
    ]
    
    for query in test_queries:
        logger.info(f"\n=== æœç´¢æ’­æ”¾URL: {query} ===")
        
        # æ‰‹åŠ¨æ„å»ºæµ‹è¯•URLéªŒè¯æ ¼å¼
        test_url = source._build_correct_search_url(query)
        logger.info(f"æµ‹è¯•URL: {test_url}")
        
        request = MediaFetchRequest(
            subject_names=[query],
            episode_sort=EpisodeSort(1)
        )
        
        try:
            matches = list(source.fetch(request))
            
            if matches:
                logger.success(f"âœ… æ‰¾åˆ° {len(matches)} ä¸ªæ’­æ”¾é“¾æ¥:")
                for i, match in enumerate(matches[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                    title = match.media.original_title
                    url = match.media.download.url if match.media.download else ""
                    logger.info(f"  {i+1}. {title}")
                    logger.info(f"     æ’­æ”¾URL: {url}")
                
                # æ‰¾åˆ°æ’­æ”¾é“¾æ¥å°±åœæ­¢æµ‹è¯•
                logger.success(f"ğŸ‰ æˆåŠŸè·å– {query} çš„æ’­æ”¾é“¾æ¥!")
                return True
                
            else:
                logger.warning(f"æœªæ‰¾åˆ° '{query}' çš„æ’­æ”¾é“¾æ¥")
                
        except Exception as e:
            logger.error(f"æœç´¢ '{query}' æ—¶å‡ºé”™: {e}")
    
    logger.warning("âŒ æœªèƒ½æ‰¾åˆ°ä»»ä½•æ’­æ”¾é“¾æ¥")
    return False


def analyze_play_url_issue():
    """åˆ†ææ’­æ”¾URLè·å–é—®é¢˜"""
    logger.info("\n=== æ’­æ”¾URLè·å–é—®é¢˜åˆ†æ ===")
    
    issues = [
        "1. ç½‘ç«™æœç´¢è¿”å›çš„å¯èƒ½æ˜¯åˆ†ç±»é¡µé¢è€Œä¸æ˜¯å…·ä½“åŠ¨æ¼«",
        "2. éœ€è¦è¿›ä¸€æ­¥è®¿é—®åˆ†ç±»é¡µé¢æ‰èƒ½è·å–å…·ä½“çš„æ’­æ”¾é“¾æ¥",
        "3. æ’­æ”¾é“¾æ¥å¯èƒ½éœ€è¦äºŒæ¬¡è§£æï¼ˆå…ˆè·å–åŠ¨æ¼«é¡µé¢ï¼Œå†è·å–æ’­æ”¾é“¾æ¥ï¼‰",
        "4. ç½‘ç«™å¯èƒ½éœ€è¦ç‰¹å®šçš„æœç´¢å…³é”®è¯æ‰æœ‰ç»“æœ"
    ]
    
    for issue in issues:
        logger.info(issue)
    
    logger.info("\nè§£å†³æ–¹æ¡ˆ:")
    solutions = [
        "1. å°è¯•è®¿é—®åˆ†ç±»é¡µé¢ï¼ˆå¦‚2025å¹´æ—¥ç•ªï¼‰è·å–å…·ä½“åŠ¨æ¼«åˆ—è¡¨",
        "2. å®ç°ä¸¤é˜¶æ®µæœç´¢ï¼šæœç´¢ -> åŠ¨æ¼«é¡µé¢ -> æ’­æ”¾é“¾æ¥",
        "3. åˆ†æå…·ä½“çš„åŠ¨æ¼«è¯¦æƒ…é¡µé¢ç»“æ„", 
        "4. å¯»æ‰¾ç½‘ç«™ä¸­ç¡®å®å­˜åœ¨çš„åŠ¨æ¼«è¿›è¡Œæµ‹è¯•"
    ]
    
    for solution in solutions:
        logger.info(solution)


def test_category_page():
    """æµ‹è¯•ç›´æ¥è®¿é—®åˆ†ç±»é¡µé¢è·å–æ’­æ”¾é“¾æ¥"""
    logger.info("\n=== æµ‹è¯•åˆ†ç±»é¡µé¢æ’­æ”¾é“¾æ¥æå– ===")
    
    # åŸºäºä¹‹å‰çš„æˆåŠŸç»“æœï¼Œå°è¯•è®¿é—®åˆ†ç±»é¡µé¢
    category_urls = [
        "https://anime.girigirilove.com/show/2-----------2025/",  # 2025å¹´æ—¥ç•ª
        "https://anime.girigirilove.com/show/2-----------2024/",  # 2024å¹´æ—¥ç•ª
    ]
    
    for category_url in category_urls:
        logger.info(f"\nè®¿é—®åˆ†ç±»é¡µé¢: {category_url}")
        
        try:
            import requests
            from bs4 import BeautifulSoup
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(category_url, headers=headers, timeout=15)
            logger.info(f"å“åº”çŠ¶æ€: {response.status_code}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å¯»æ‰¾å…·ä½“çš„åŠ¨æ¼«é“¾æ¥
                anime_links = soup.find_all('a', href=lambda x: x and '/GV' in x if x else False)
                
                if anime_links:
                    logger.success(f"æ‰¾åˆ° {len(anime_links)} ä¸ªåŠ¨æ¼«æ’­æ”¾é“¾æ¥:")
                    for i, link in enumerate(anime_links[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        logger.info(f"  {i+1}. {text} -> {href}")
                    
                    return True
                else:
                    logger.warning("åœ¨åˆ†ç±»é¡µé¢ä¸­æœªæ‰¾åˆ°æ’­æ”¾é“¾æ¥")
            
        except Exception as e:
            logger.error(f"è®¿é—®åˆ†ç±»é¡µé¢æ—¶å‡ºé”™: {e}")
    
    return False


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ girigirilove æ’­æ”¾URLè·å–ä¸“ç”¨é…ç½®")
    logger.info("ç›®æ ‡ï¼šè·å–å®é™…çš„æ’­æ”¾URLè€Œä¸æ˜¯åˆ†ç±»é¡µé¢")
    logger.info("=" * 60)
    
    # æµ‹è¯•æ’­æ”¾URLæå–
    success = test_playurl_extraction()
    
    if not success:
        # å¦‚æœç›´æ¥æœç´¢å¤±è´¥ï¼Œå°è¯•è®¿é—®åˆ†ç±»é¡µé¢
        logger.info("\nç›´æ¥æœç´¢æœªæˆåŠŸï¼Œå°è¯•åˆ†ç±»é¡µé¢æ–¹æ³•...")
        test_category_page()
    
    # åˆ†æé—®é¢˜
    analyze_play_url_issue()
    
    logger.info("\n" + "=" * 60)
    logger.success("æ’­æ”¾URLé…ç½®æµ‹è¯•å®Œæˆï¼")
    logger.info("ä¸‹ä¸€æ­¥å»ºè®®ï¼š")
    logger.info("1. å¦‚æœæ‰¾åˆ°æ’­æ”¾é“¾æ¥ï¼Œè¿›ä¸€æ­¥è§£æå‰§é›†é¡µé¢")
    logger.info("2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè€ƒè™‘ç›´æ¥åˆ†æåŠ¨æ¼«åˆ†ç±»é¡µé¢")
    logger.info("3. å®ç°ä¸¤é˜¶æ®µçˆ¬å–ï¼šåˆ†ç±»é¡µé¢ -> æ’­æ”¾é¡µé¢")


if __name__ == "__main__":
    main()