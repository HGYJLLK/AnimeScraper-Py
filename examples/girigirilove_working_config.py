#!/usr/bin/env python3
"""
girigirilove å¯å·¥ä½œé…ç½®
åŸºäºçœŸå®URLæ ¼å¼å’ŒHTMLç»“æ„åˆ†æ
"""

import sys
import os
from urllib.parse import quote

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from web_scraper.core import SelectorMediaSource
from web_scraper.models import (
    SelectorSearchConfig, MediaFetchRequest, EpisodeSort,
    SelectorSubjectFormatConfig, SelectorChannelFormatConfig, MatchVideoConfig
)
from web_scraper.utils.logger import logger


def create_working_config():
    """
    åŸºäºå‘ç°çš„çœŸå®æ ¼å¼åˆ›å»ºé…ç½®
    URLæ ¼å¼: https://anime.girigirilove.com/search/-------------/?wd=è¿›å‡»çš„å·¨äºº
    """
    return SelectorSearchConfig(
        # çœŸå®çš„æœç´¢URLæ ¼å¼
        search_url="https://anime.girigirilove.com/search/-------------/?wd={keyword}",
        
        # æœç´¢è®¾ç½®
        search_use_only_first_word=False,  # ä¿ç•™å®Œæ•´å…³é”®è¯
        search_remove_special=False,       # ä¿ç•™ç‰¹æ®Šå­—ç¬¦ï¼Œå› ä¸ºæ˜¯æŸ¥è¯¢å‚æ•°
        search_use_subject_names_count=2,  # å°è¯•å¤šä¸ªåç§°
        
        # è¯·æ±‚è®¾ç½®
        request_interval_seconds=3.0,      # 3ç§’é—´éš”
        
        # ä¸»é¢˜æ ¼å¼é…ç½® - åŸºäºåˆ†æçš„HTMLç»“æ„
        subject_format_config=SelectorSubjectFormatConfig(
            # æœç´¢ç»“æœé¡¹å®¹å™¨
            subject_selector=".mac-list li, .mac-list .mac-item",
            # åŠ¨æ¼«æ ‡é¢˜é€‰æ‹©å™¨
            name_selector=".mac-list-title a, .mac-list-title, .title a, .title",
            # è¯¦æƒ…é¡µé“¾æ¥é€‰æ‹©å™¨  
            url_selector=".mac-list-title a, a[href*='/show/'], a[href*='/G']"
        ),
        
        # é¢‘é“æ ¼å¼é…ç½® - å‰§é›†é¡µé¢
        channel_format_config=SelectorChannelFormatConfig(
            # å‰§é›†å®¹å™¨ï¼ˆéœ€è¦è¿›ä¸€æ­¥åˆ†æå‰§é›†é¡µé¢ï¼‰
            episode_selector=".mac-list-item, .episode-item, .play-list li, .playlist li",
            # å‰§é›†æ ‡é¢˜
            name_selector=".mac-list-title, .episode-title, .title, a",
            # æ’­æ”¾é“¾æ¥
            url_selector="a, .play-btn, .episode-link"
        ),
        
        # è§†é¢‘åŒ¹é…é…ç½®
        match_video=MatchVideoConfig(
            enable_nested_url=True,
            match_video_url=r"(\.mp4|\.m3u8|/play/|/video/|stream|watch)",
            cookies="",
        ),
        
        default_resolution="1080P", 
        default_subtitle_language="CHS"
    )


class GirigiriloveMediaSource(SelectorMediaSource):
    """
    girigirilove ä¸“ç”¨åª’ä½“æº
    å¤„ç†URLç¼–ç å’Œç‰¹æ®Šæ ¼å¼
    """
    
    def __init__(self, media_source_id: str, config: SelectorSearchConfig, session=None):
        super().__init__(media_source_id, config, session)
    
    def _encode_search_keyword(self, keyword):
        """å¯¹æœç´¢å…³é”®è¯è¿›è¡ŒURLç¼–ç """
        # URLç¼–ç ä¸­æ–‡å­—ç¬¦
        encoded = quote(keyword, safe='')
        logger.debug(f"å…³é”®è¯ç¼–ç : {keyword} -> {encoded}")
        return encoded
    
    async def search(self, search_config, query):
        """è¦†ç›–æœç´¢æ–¹æ³•å¤„ç†URLç¼–ç """
        original_url = search_config.search_url
        
        # å¯¹å…³é”®è¯è¿›è¡ŒURLç¼–ç 
        encoded_keyword = self._encode_search_keyword(query.subject_name)
        search_config.search_url = original_url.replace('{keyword}', encoded_keyword)
        
        logger.info(f"æœç´¢URL: {search_config.search_url}")
        
        try:
            result = await super().search(search_config, query)
            return result
        finally:
            # æ¢å¤åŸå§‹URL
            search_config.search_url = original_url


def test_working_config():
    """æµ‹è¯•å¯å·¥ä½œçš„é…ç½®"""
    logger.info("ğŸ¬ æµ‹è¯• girigirilove å¯å·¥ä½œé…ç½®")
    logger.info("=" * 60)
    
    config = create_working_config()
    source = GirigiriloveMediaSource("girigirilove-working", config)
    
    # æµ‹è¯•è¿æ¥
    logger.info("æµ‹è¯•åŸºæœ¬è¿æ¥...")
    connection_status = source.check_connection()
    logger.info(f"è¿æ¥çŠ¶æ€: {connection_status}")
    
    if connection_status != "SUCCESS":
        logger.warning("åŸºæœ¬è¿æ¥å¤±è´¥ï¼Œä½†å¯èƒ½æ˜¯æœç´¢URLæ ¼å¼é—®é¢˜ï¼Œç»§ç»­æµ‹è¯•æœç´¢åŠŸèƒ½")
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    test_queries = [
        "è¿›å‡»çš„å·¨äºº",
        "é¬¼ç­ä¹‹åˆƒ",
        "æµ·è´¼ç‹"
    ]
    
    for query in test_queries:
        logger.info(f"\n=== æœç´¢æµ‹è¯•: {query} ===")
        
        # æµ‹è¯•URLç¼–ç 
        encoded = source._encode_search_keyword(query)
        test_url = config.search_url.replace('{keyword}', encoded)
        logger.info(f"æµ‹è¯•URL: {test_url}")
        
        # åˆ›å»ºæœç´¢è¯·æ±‚
        request = MediaFetchRequest(
            subject_names=[query],
            episode_sort=EpisodeSort(1)
        )
        
        try:
            logger.info("æ‰§è¡Œæœç´¢...")
            matches = list(source.fetch(request))
            
            if matches:
                logger.success(f"âœ“ æ‰¾åˆ° {len(matches)} ä¸ªç»“æœ:")
                for i, match in enumerate(matches[:3]):
                    logger.info(f"  {i+1}. {match.media.original_title}")
                    logger.info(f"     URL: {match.media.download.url}")
                    
                # æ‰¾åˆ°ç»“æœå°±åœæ­¢ï¼Œé¿å…è¿‡å¤šè¯·æ±‚
                logger.success("æœç´¢æˆåŠŸï¼é…ç½®å·¥ä½œæ­£å¸¸ã€‚")
                break
                
            else:
                logger.warning(f"æœªæ‰¾åˆ° '{query}' çš„æœç´¢ç»“æœ")
                logger.info("å¯èƒ½åŸå› :")
                logger.info("1. CSSé€‰æ‹©å™¨éœ€è¦è°ƒæ•´")
                logger.info("2. æœç´¢å…³é”®è¯åœ¨ç½‘ç«™ä¸Šä¸å­˜åœ¨")
                logger.info("3. ç½‘ç«™è¿”å›æ ¼å¼ä¸é¢„æœŸä¸ç¬¦")
                
        except Exception as e:
            logger.error(f"æœç´¢ '{query}' æ—¶å‡ºé”™: {e}")
            logger.info("ç»§ç»­æµ‹è¯•ä¸‹ä¸€ä¸ªå…³é”®è¯...")
    
    logger.info(f"\n{'='*60}")
    logger.success("é…ç½®æµ‹è¯•å®Œæˆï¼")


def debug_search_page():
    """è°ƒè¯•æœç´¢é¡µé¢ç»“æ„"""
    logger.info("\n=== è°ƒè¯•æœç´¢é¡µé¢ç»“æ„ ===")
    
    import requests
    from bs4 import BeautifulSoup
    
    # æµ‹è¯•æœç´¢é¡µé¢
    test_url = "https://anime.girigirilove.com/search/-------------/?wd=è¿›å‡»çš„å·¨äºº"
    
    logger.info(f"è·å–é¡µé¢: {test_url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'
        }
        
        response = requests.get(test_url, headers=headers, timeout=15)
        logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å¯èƒ½çš„æœç´¢ç»“æœå®¹å™¨
            possible_containers = [
                soup.find_all(class_=lambda x: x and 'mac-list' in x if x else False),
                soup.find_all(class_=lambda x: x and 'list' in x if x else False),
                soup.find_all(class_=lambda x: x and 'item' in x if x else False),
                soup.find_all('li'),
                soup.find_all(class_=lambda x: x and 'card' in x if x else False)
            ]
            
            for i, containers in enumerate(possible_containers):
                if containers:
                    logger.info(f"æ‰¾åˆ°å®¹å™¨ç±»å‹ {i+1}: {len(containers)} ä¸ªå…ƒç´ ")
                    for j, container in enumerate(containers[:2]):
                        classes = container.get('class', [])
                        logger.info(f"  å®¹å™¨ {j+1}: {classes}")
                        
                        # æŸ¥æ‰¾å…¶ä¸­çš„é“¾æ¥å’Œæ–‡æœ¬
                        links = container.find_all('a')
                        if links:
                            for k, link in enumerate(links[:2]):
                                href = link.get('href', '')
                                text = link.get_text(strip=True)
                                logger.info(f"    é“¾æ¥ {k+1}: {href} - {text[:30]}")
            
            # æŸ¥æ‰¾æ ‡é¢˜ç›¸å…³å…ƒç´ 
            title_elements = soup.find_all(class_=lambda x: x and 'title' in ' '.join(x).lower() if x else False)
            if title_elements:
                logger.info(f"æ‰¾åˆ° {len(title_elements)} ä¸ªæ ‡é¢˜å…ƒç´ :")
                for i, elem in enumerate(title_elements[:3]):
                    classes = elem.get('class', [])
                    text = elem.get_text(strip=True)
                    logger.info(f"  æ ‡é¢˜ {i+1}: {classes} - {text[:50]}")
            
        else:
            logger.error(f"é¡µé¢è®¿é—®å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        logger.error(f"è°ƒè¯•é¡µé¢æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ girigirilove å®Œæ•´æµ‹è¯•")
    logger.info("åŸºäºå‘ç°çš„çœŸå®URLæ ¼å¼è¿›è¡Œæµ‹è¯•")
    
    # é¦–å…ˆè°ƒè¯•é¡µé¢ç»“æ„
    debug_search_page()
    
    # ç„¶åæµ‹è¯•é…ç½®
    test_working_config()


if __name__ == "__main__":
    main()