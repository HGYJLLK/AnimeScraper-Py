#!/usr/bin/env python3
"""
é«˜çº§ girigirilove ç½‘ç«™åˆ†æå·¥å…·
å¤„ç†403é”™è¯¯å’Œå¯»æ‰¾çœŸå®çš„æœç´¢API
"""

import sys
import os
import requests
from bs4 import BeautifulSoup
import re
import json
import time
from urllib.parse import urljoin, parse_qs, urlparse

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from web_scraper.utils.logger import logger


class GirigiriloveAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://anime.girigirilove.com"
        
        # æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨ç¯å¢ƒ
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        self.session.headers.update(self.headers)
    
    def analyze_main_page(self):
        """æ·±åº¦åˆ†æä¸»é¡µç»“æ„"""
        logger.info("=== æ·±åº¦åˆ†æä¸»é¡µç»“æ„ ===")
        
        try:
            response = self.session.get(self.base_url, timeout=15)
            logger.info(f"ä¸»é¡µçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                logger.error("ä¸»é¡µè®¿é—®å¤±è´¥")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # åˆ†ææœç´¢è¡¨å•
            forms = soup.find_all('form')
            logger.info(f"æ‰¾åˆ° {len(forms)} ä¸ªè¡¨å•")
            
            for i, form in enumerate(forms):
                action = form.get('action', '')
                method = form.get('method', 'GET')
                logger.info(f"è¡¨å• {i+1}: method={method}, action={action}")
                
                # åˆ†æè¡¨å•ä¸­çš„è¾“å…¥å­—æ®µ
                inputs = form.find_all(['input', 'select', 'textarea'])
                for inp in inputs:
                    name = inp.get('name', '')
                    inp_type = inp.get('type', inp.name)
                    placeholder = inp.get('placeholder', '')
                    if name or placeholder:
                        logger.info(f"  è¾“å…¥å­—æ®µ: name='{name}', type={inp_type}, placeholder='{placeholder}'")
            
            # å¯»æ‰¾JavaScriptä¸­çš„APIç«¯ç‚¹
            scripts = soup.find_all('script')
            api_patterns = [
                r'["\']https?://[^"\']*api[^"\']*["\']',
                r'["\']https?://[^"\']*search[^"\']*["\']',
                r'["\'][/]api[^"\']*["\']',
                r'["\'][/]search[^"\']*["\']',
                r'url\s*:\s*["\'][^"\']+["\']',
                r'fetch\s*\(\s*["\'][^"\']+["\']'
            ]
            
            found_apis = set()
            for script in scripts:
                if script.string:
                    for pattern in api_patterns:
                        matches = re.findall(pattern, script.string, re.IGNORECASE)
                        for match in matches:
                            # æ¸…ç†åŒ¹é…ç»“æœ
                            clean_url = match.strip('"\'')
                            if clean_url and not clean_url.startswith('data:'):
                                found_apis.add(clean_url)
            
            if found_apis:
                logger.success("åœ¨JavaScriptä¸­å‘ç°å¯èƒ½çš„APIç«¯ç‚¹:")
                for api in sorted(found_apis):
                    logger.info(f"  {api}")
            
            return soup
            
        except Exception as e:
            logger.error(f"åˆ†æä¸»é¡µå¤±è´¥: {e}")
            return None
    
    def find_search_mechanism(self, soup):
        """å¯»æ‰¾æœç´¢æœºåˆ¶"""
        logger.info("\n=== å¯»æ‰¾æœç´¢æœºåˆ¶ ===")
        
        if not soup:
            return []
        
        search_methods = []
        
        # æ–¹æ³•1: åˆ†ææœç´¢è¡¨å•
        search_forms = soup.find_all('form', class_=lambda x: x and 'search' in ' '.join(x).lower() if x else False)
        if not search_forms:
            # å¯»æ‰¾åŒ…å«æœç´¢è¾“å…¥æ¡†çš„è¡¨å•
            search_inputs = soup.find_all('input', {'name': lambda x: x and 'search' in x.lower() if x else False})
            search_inputs.extend(soup.find_all('input', {'class': lambda x: x and any('search' in cls for cls in x) if x else False}))
            
            for inp in search_inputs:
                form = inp.find_parent('form')
                if form and form not in search_forms:
                    search_forms.append(form)
        
        for form in search_forms:
            action = form.get('action', '')
            method = form.get('method', 'GET').upper()
            
            # æ„å»ºå®Œæ•´çš„action URL
            if action:
                if action.startswith('/'):
                    full_action = urljoin(self.base_url, action)
                elif action.startswith('http'):
                    full_action = action
                else:
                    full_action = urljoin(self.base_url + '/', action)
            else:
                full_action = self.base_url
            
            search_methods.append({
                'type': 'form',
                'method': method,
                'action': full_action,
                'form': form
            })
            
            logger.info(f"å‘ç°æœç´¢è¡¨å•: {method} {full_action}")
        
        # æ–¹æ³•2: æµ‹è¯•å¸¸è§æœç´¢ç«¯ç‚¹
        common_endpoints = [
            '/search',
            '/api/search',
            '/s',
            '/find',
            '/query'
        ]
        
        for endpoint in common_endpoints:
            test_url = self.base_url + endpoint
            search_methods.append({
                'type': 'endpoint',
                'method': 'GET',
                'action': test_url + '?q={keyword}'
            })
        
        return search_methods
    
    def test_search_methods(self, search_methods):
        """æµ‹è¯•æœç´¢æ–¹æ³•"""
        logger.info("\n=== æµ‹è¯•æœç´¢æ–¹æ³• ===")
        
        test_keyword = "test"
        successful_methods = []
        
        for method_info in search_methods:
            logger.info(f"\næµ‹è¯•: {method_info['method']} {method_info['action']}")
            
            try:
                if method_info['type'] == 'form':
                    # åˆ†æè¡¨å•å‚æ•°
                    form = method_info['form']
                    data = {}
                    
                    for inp in form.find_all(['input', 'select', 'textarea']):
                        name = inp.get('name')
                        if name:
                            if inp.get('type') == 'hidden':
                                data[name] = inp.get('value', '')
                            elif 'search' in name.lower() or 'keyword' in name.lower() or 'q' in name.lower():
                                data[name] = test_keyword
                            elif inp.get('type') in ['text', 'search']:
                                data[name] = test_keyword
                    
                    logger.info(f"  è¡¨å•æ•°æ®: {data}")
                    
                    if method_info['method'] == 'POST':
                        response = self.session.post(method_info['action'], data=data, timeout=10)
                    else:
                        response = self.session.get(method_info['action'], params=data, timeout=10)
                
                else:  # endpoint
                    test_url = method_info['action'].replace('{keyword}', test_keyword)
                    response = self.session.get(test_url, timeout=10)
                
                logger.info(f"  çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '').lower()
                    
                    if 'json' in content_type:
                        try:
                            data = response.json()
                            logger.success(f"  âœ“ JSONå“åº”æˆåŠŸ: {str(data)[:100]}...")
                            successful_methods.append({
                                **method_info,
                                'response_type': 'json',
                                'test_response': data
                            })
                        except:
                            logger.warning("  JSONè§£æå¤±è´¥")
                    else:
                        # HTMLå“åº”
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                        possible_results = soup.find_all(['div', 'li', 'article'], 
                                                       class_=lambda x: x and any(word in ' '.join(x).lower() 
                                                       for word in ['item', 'result', 'card', 'anime', 'show']) if x else False)
                        
                        if possible_results:
                            logger.success(f"  âœ“ HTMLæœç´¢é¡µé¢ï¼Œæ‰¾åˆ° {len(possible_results)} ä¸ªå¯èƒ½çš„ç»“æœå®¹å™¨")
                            successful_methods.append({
                                **method_info,
                                'response_type': 'html',
                                'result_containers': len(possible_results)
                            })
                        else:
                            logger.warning("  HTMLé¡µé¢ï¼Œä½†æœªæ‰¾åˆ°æ˜æ˜¾çš„æœç´¢ç»“æœ")
                
                elif response.status_code == 403:
                    logger.warning("  403 Forbidden - å¯èƒ½éœ€è¦é¢å¤–çš„è®¤è¯æˆ–headers")
                elif response.status_code == 404:
                    logger.warning("  404 Not Found - ç«¯ç‚¹ä¸å­˜åœ¨")
                else:
                    logger.warning(f"  å…¶ä»–é”™è¯¯: {response.status_code}")
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logger.error(f"  è¯·æ±‚å¤±è´¥: {e}")
        
        return successful_methods
    
    def analyze_show_page_structure(self):
        """åˆ†æåŠ¨æ¼«è¯¦æƒ…é¡µç»“æ„"""
        logger.info("\n=== åˆ†æåŠ¨æ¼«è¯¦æƒ…é¡µç»“æ„ ===")
        
        # åŸºäºä¹‹å‰å‘ç°çš„é“¾æ¥æ ¼å¼ï¼Œå°è¯•è®¿é—®ä¸€ä¸ªåŠ¨æ¼«é¡µé¢
        test_urls = [
            f"{self.base_url}/show/1",
            f"{self.base_url}/show/2", 
            f"{self.base_url}/show/100"
        ]
        
        for test_url in test_urls:
            logger.info(f"æµ‹è¯•åŠ¨æ¼«é¡µé¢: {test_url}")
            
            try:
                response = self.session.get(test_url, timeout=10)
                logger.info(f"  çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # å¯»æ‰¾å‰§é›†åˆ—è¡¨
                    episode_containers = soup.find_all(['div', 'ul', 'ol'], 
                                                     class_=lambda x: x and any(word in ' '.join(x).lower() 
                                                     for word in ['episode', 'ep', 'playlist', 'video']) if x else False)
                    
                    if episode_containers:
                        logger.success(f"  æ‰¾åˆ° {len(episode_containers)} ä¸ªå¯èƒ½çš„å‰§é›†å®¹å™¨")
                        
                        for i, container in enumerate(episode_containers[:2]):
                            logger.info(f"    å®¹å™¨ {i+1}: class={container.get('class', [])}")
                            
                            # å¯»æ‰¾å®¹å™¨å†…çš„é“¾æ¥
                            links = container.find_all('a')
                            if links:
                                logger.info(f"      åŒ…å« {len(links)} ä¸ªé“¾æ¥")
                                for j, link in enumerate(links[:3]):
                                    href = link.get('href', '')
                                    text = link.get_text(strip=True)
                                    logger.info(f"        é“¾æ¥ {j+1}: {href} - {text[:30]}")
                    
                    return soup
                    
            except Exception as e:
                logger.error(f"  è®¿é—®å¤±è´¥: {e}")
        
        return None
    
    def generate_config_recommendations(self, successful_methods):
        """ç”Ÿæˆé…ç½®å»ºè®®"""
        logger.info("\n=== é…ç½®å»ºè®® ===")
        
        if not successful_methods:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœç´¢æ–¹æ³•ï¼Œå»ºè®®:")
            logger.info("1. æ£€æŸ¥ç½‘ç«™æ˜¯å¦éœ€è¦ç™»å½•")
            logger.info("2. æŸ¥çœ‹ç½‘ç«™æ˜¯å¦ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½")
            logger.info("3. å°è¯•ä½¿ç”¨Seleniumç­‰å·¥å…·æ¨¡æ‹Ÿæµè§ˆå™¨")
            return
        
        for i, method in enumerate(successful_methods):
            logger.success(f"\næ–¹æ³• {i+1}:")
            logger.info(f"  ç±»å‹: {method['type']}")
            logger.info(f"  HTTPæ–¹æ³•: {method['method']}")
            logger.info(f"  URL: {method['action']}")
            
            if method['response_type'] == 'json':
                logger.info("  å“åº”ç±»å‹: JSON API")
                logger.info("  å»ºè®®: ä½¿ç”¨APIæ¥å£ï¼Œå¯èƒ½éœ€è¦è‡ªå®šä¹‰è§£æé€»è¾‘")
            else:
                logger.info("  å“åº”ç±»å‹: HTMLé¡µé¢")
                logger.info("  å»ºè®®: ä½¿ç”¨ç°æœ‰çš„CSSé€‰æ‹©å™¨é…ç½®")
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info("ğŸ” å¼€å§‹å®Œæ•´çš„ç½‘ç«™åˆ†æ")
        logger.info("=" * 60)
        
        # 1. åˆ†æä¸»é¡µ
        soup = self.analyze_main_page()
        
        # 2. å¯»æ‰¾æœç´¢æœºåˆ¶
        search_methods = self.find_search_mechanism(soup)
        
        # 3. æµ‹è¯•æœç´¢æ–¹æ³•
        successful_methods = self.test_search_methods(search_methods)
        
        # 4. åˆ†æåŠ¨æ¼«é¡µé¢ç»“æ„
        self.analyze_show_page_structure()
        
        # 5. ç”Ÿæˆé…ç½®å»ºè®®
        self.generate_config_recommendations(successful_methods)
        
        logger.info("\n" + "=" * 60)
        logger.success("åˆ†æå®Œæˆï¼")
        
        return successful_methods


def main():
    analyzer = GirigiriloveAnalyzer()
    successful_methods = analyzer.run_full_analysis()
    
    if successful_methods:
        logger.info("\nä¸‹ä¸€æ­¥: æ ¹æ®æˆåŠŸçš„æ–¹æ³•åˆ›å»ºçˆ¬è™«é…ç½®")
    else:
        logger.info("\nä¸‹ä¸€æ­¥: è€ƒè™‘ä½¿ç”¨æ›´é«˜çº§çš„å·¥å…·æˆ–æ‰‹åŠ¨åˆ†æç½‘ç«™")


if __name__ == "__main__":
    main()